{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 : Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Apply the simplest variant of gradient descent (\"vanilla gradient descent\", VGD) to two simple use cases:\n",
    "\n",
    "a. Linear Regression: Test by comparing with the solution of the normal equations.<br>\n",
    "b. Rosenbrock Function: Explore the behavior of the method with different initial values and learning rates.<br>\n",
    "\n",
    "The formulas for the gradient for the two use cases can be found on the slides. These will be helpful to implement vanilla gradient descent. For validating that the formula and the implementation is correct, you also need to implement gradient checking. \n",
    "\n",
    "**Overview**\n",
    "\n",
    "| Exercise | Content | Punkte |\n",
    "| :--- | :--- | :---: |\n",
    "| Exercise 1 | Vanilla Gradient Descent (VGD) | min req for \"pass\", 1 bonus point |\n",
    "| Exercise 2 | Application of VGD to Linear Regression | min req for \"pass\" |\n",
    "| Exercise 3 | Analysis of VGD applied to Linear Regression | min req for \"pass\", 1 bonus point |\n",
    "| Exercise 4 | Application of VGD to Rosenbrock | min req for \"pass\" |\n",
    "| Exercise 5 | Analysis of VGD applied to Rosenbrock | bonus points: 2 |\n",
    "\n",
    "For convenience, I have added some unit test cells (marked as **<span style=\"color:green\">TEST</span>**). If their execution do result in a **Python-Error** (e.g. Assertion Error) then something is probably wrong with your implementation. Don't modify the test cells.\n",
    "\n",
    "Please send your solution by email to [Martin Melchior](mailto:martin.melchior@fhnw.ch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations, Imports, Plotting\n",
    "\n",
    "Implement the methods below in <code>Numpy</code>. \n",
    "\n",
    "**Important Remark:** The function variables $x$ are vectors. With fixed numerical values this translates to numpy-arrays. The shape of the numpy arrays Numpy Arrays ist je nach Use Case unterschiedlich. Wir w√§hlen in den Use Cases Lineare Regression und Rosenbrock Shapes $(n,1)$, wobei $n$ die Anzahl Variablen bezeichnet.\n",
    "\n",
    "For plotting we will use <code>Matplotlib</code>. \n",
    "\n",
    "Accordingly, we only need the following two imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking progress in the optimization process, we want to see how the cost and the learning speed progresses. The following function can be used to achieve that: <code>learningcurve_plots</code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningcurve_plots(cost_hist, learning_speed_hist, logy=False):\n",
    "    \"\"\"\n",
    "    cost_hist -- history of cost values, as np-array of shape (T,1)\n",
    "    learning_speed_hist -- history of learning speed values, as np-array of shape (T,1)\n",
    "    logy -- if set to True will plot the y axis at logarithmic scale\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    T = len(cost_hist)\n",
    "    if logy:\n",
    "        plt.semilogy(np.arange(T),cost_hist,'b-')\n",
    "    else:\n",
    "        plt.plot(np.arange(T),cost_hist,'b-')\n",
    "    plt.title(\"Cost\")\n",
    "        \n",
    "    plt.figure(2)\n",
    "    T = len(learning_speed_hist)\n",
    "    if logy:\n",
    "        plt.semilogy(np.arange(T),learning_speed_hist,'g-')\n",
    "    else:\n",
    "        plt.plot(np.arange(T),learning_speed_hist,'g-')\n",
    "    plt.title(\"Learning Speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla Gradient Descent \n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### <span style=\"color:blue\">EXERCISE 1</span> \n",
    "\n",
    "\n",
    "Implement a function for the computation of Vanilla Gradient Descent.\n",
    "The functions should adopt the signature as described below. For passing other parameters to the cost and gradient function use \"keyword arguments\" (<code>kwargs</code>) that can be passed forward to the calculation of the cost and gradient. \n",
    "\n",
    "The implementation can be tested (to some extent) with the subsequent test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_gradient_descent( cost, gradient_cost, learningrate, x_initial, max_iter, eps=1.0e-4, **kwargs):\n",
    "    \"\"\"\n",
    "    Performs plain vanilla gradient descent for the cost function with given gradient-function. The variables x of the function are provided as np-arrays.\n",
    "    \n",
    "    Arguments:\n",
    "    cost -- cost function as a function object that accepts as input a np-array (shape of x) and returns a real number.\n",
    "    gradient_cost -- gradient of the cost function that accepts as input a np-array (shape of x) and returns a np-array (shape of x).\n",
    "    learningrate -- learning rate\n",
    "    x_initial -- initial value, np-array with shape of x\n",
    "    max_iter -- maximum number of iterations\n",
    "    eps -- tolerance used for formulating the stopping criterium: here, formulated in terms of the length of the gradient.\n",
    "    kwargs -- further arguments that can passed forward to the call of the cost and gradient function.\n",
    "    \n",
    "    Returns:\n",
    "    xopt -- the optimum value x where the cost function is minimized (within the given tolerance): np-array with shape of x\n",
    "    cost_hist -- history of the values of the cost function seen during the iteration loop: np-array of shape (T,1) where T is the number of iteration needed.\n",
    "    learning_speed_hist -- history of the learning speed where the learning speed given by the norm of the difference between subsequent x-values: np-array of shape (T,1) where T is the number of iteration needed.\n",
    "    \"\"\"\n",
    "    # START YOUR CODE\n",
    "\n",
    "    iterations = 0\n",
    "    x_current = x_initial\n",
    "        \n",
    "    ch_list = [gradient_cost(x_current, kwargs['a'], kwargs['b'])]\n",
    "    lsh_list = [np.linalg.norm(learningrate*(gradient_cost(x_current, kwargs['a'], kwargs['b'])))]\n",
    "\n",
    "    \n",
    "    while((lsh_list[iterations] > eps) and (iterations < max_iter)):\n",
    "        x_current = x_current - learningrate*(gradient_cost(x_current, kwargs['a'], kwargs['b']))\n",
    "               \n",
    "        ch_list.append(cost(x_current, kwargs['a'], kwargs['b']))\n",
    "        lsh_list.append(np.linalg.norm(learningrate*(gradient_cost(x_current, kwargs['a'], kwargs['b']))))\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    \n",
    "    cost_hist = np.array(ch_list)\n",
    "    learning_speed_hist = np.array(lsh_list)\n",
    "    return x_current, cost_hist, learning_speed_hist\n",
    "    \n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a248ac49f795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mxopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_speed_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvanilla_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gradient_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# TEST\n",
    "#######\n",
    "\n",
    "def test_cost(x, a, b):\n",
    "    \"\"\"\n",
    "    x - the variables to be optimized\n",
    "    a - parameter (treated by vanilla_gradient_descent as kwargs)\n",
    "    b - parameter (treated by vanilla_gradient_descent as kwargs)\n",
    "    \"\"\"\n",
    "    return (a*x[0,0]**2 + b*x[1,0]**2)/2.0\n",
    "\n",
    "def test_gradient_cost(x, a, b):\n",
    "    \"\"\"\n",
    "    x - the variables to be optimized\n",
    "    a - parameter (treated by vanilla_gradient_descent as kwargs)\n",
    "    b - parameter (treated by vanilla_gradient_descent as kwargs)\n",
    "    \"\"\"\n",
    "    return np.array([a,b]).reshape(2,1)*x\n",
    "\n",
    "learningrate = 0.1\n",
    "x0 = np.array([10,10]).reshape(2,1)\n",
    "\n",
    "xopt, cost_hist, learning_speed_hist = vanilla_gradient_descent(test_cost, test_gradient_cost, learningrate, x0, 1000, eps=1.0e-8, a=1, b=2)\n",
    "T = cost_hist.size-1\n",
    "print(\"Number of iterations: %i, final cost: %6.5f, final speed: %6.5f\"%(T, cost_hist[-1,0], learning_speed_hist[-1,0]))\n",
    "learningcurve_plots(cost_hist, learning_speed_hist)\n",
    "\n",
    "np.testing.assert_array_equal((2,1), xopt.shape)\n",
    "np.testing.assert_array_equal((T+1,1), cost_hist.shape)\n",
    "np.testing.assert_array_equal((T,1), learning_speed_hist.shape)\n",
    "np.testing.assert_array_almost_equal(np.linalg.norm(xopt), 0.0, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Application to Linear Regression\n",
    "\n",
    "Apply Vanilla Gradient Descent to the cost function used for linear regression.<br>\n",
    "Test the method by comparing with the solution of the normal equations on the basis of the dataset studied in the lecture and exercise 2 of worksheet week 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Data\n",
    "def load_regression_data():\n",
    "    x = np.array([1.0,2.0,3.0,2.5,3.0,3.0,4.0,4.0,4.5,5.0,5.0,6.0,6.0,7.0,8.0,8.0,9.0,9.0])\n",
    "    N = x.size\n",
    "    x = x.reshape(N,1)\n",
    "    y = np.array([2.0,2.0,4.0,4.0,3.0,4.0,4.0,5.0,6.0,5.0,6.0,7.0,9.0,8.0,7.0,8.0,8.0,9.0]).reshape(N,1)\n",
    "    return x,y\n",
    "\n",
    "x,y = load_regression_data()\n",
    "plt.plot(x,y,'b+')\n",
    "\n",
    "def design_matrix(x):\n",
    "    X = np.stack((x**0, x), axis=1)\n",
    "    X = X.squeeze()\n",
    "    return X\n",
    "    \n",
    "X = design_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">EXERCISE 2</span> \n",
    "\n",
    "Implement the cost function and its gradient for a general linear regression problem with design matrix $X$ and target value vector $y$.\n",
    "\n",
    "As cost function we use the average square distance between target values $y^{(i)}$ and values predicted by the model $\\hat{y}^{(i)}$. \n",
    "\n",
    "Don't get confused about the notation and what needs to get optimized, i.e. the linear regression parameters $\\alpha$. The data points $x,y$ and the design matrix are considered as additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regression_cost(alpha, X, y):\n",
    "    \"\"\"\n",
    "    Cost function for the linear regression problem with affine linear model.\n",
    "\n",
    "    Arguments:    \n",
    "    alpha -- array of parameters to be optimized: np-array of shape (m,1)\n",
    "    X -- design matrix, array of shape (N, m)\n",
    "    y -- regression result, array of shape (N,1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function for the given parameter array\n",
    "    \"\"\"\n",
    "    # START YOUR CODE\n",
    "\n",
    "    yhat = np.dot(X,alpha)\n",
    "    \n",
    "    return (np.square(np.linalg.norm(y-yhat))/(2*(len(y))))\n",
    "\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    \n",
    "def lin_regression_gradient_cost(alpha, X, y):\n",
    "    \"\"\"\n",
    "    Gradient of the cost function for the linear regression problem with affine linear model.\n",
    "    Arguments:\n",
    "    alpha -- array of parameters to be optimized: np-array of shape (m,1)\n",
    "    X -- design matrix, array of shape (N, m)\n",
    "    y -- regression result, array of shape (N,1)\n",
    "    \n",
    "    Returns:\n",
    "    gradient_cost -- the gradient of the cost function for the given parameter array\n",
    "    \"\"\"\n",
    "    # START YOUR CODE    \n",
    "    yhat = np.dot(X,alpha)\n",
    "    XT = np.transpose(X)\n",
    "    X2T = 2 * XT\n",
    "    print((np.dot(X2T, y) - np.dot(X2T,yhat))/2*(len(y)))\n",
    "    return ((np.dot(X2T, y) - np.dot(X2T,yhat))/2*(len(y)))\n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    \n",
    "def optimized_parameters_exact(X,y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- design matrix, array of shape (N, m)\n",
    "    y -- regression result, array of shape (N,1)\n",
    "\n",
    "    Returns:\n",
    "    alpha_opt -- the optimized parameters by solving the normal equations\n",
    "    \"\"\"\n",
    "    # START YOUR CODE\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-126.]\n",
      " [-891.]]\n",
      "[[-126.]\n",
      " [-891.]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 8 decimals\n\nMismatched elements: 2 / 2 (100%)\nMax absolute difference: 893.75\nMax relative difference: 1.00308642\n x: array([[0.38888889],\n       [2.75      ]])\n y: array([[-126.],\n       [-891.]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3e741257ba8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5138888888888888\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_regression_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_regression_gradient_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.38888889\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_regression_gradient_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvanilla_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin_regression_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_regression_gradient_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 8 decimals\n\nMismatched elements: 2 / 2 (100%)\nMax absolute difference: 893.75\nMax relative difference: 1.00308642\n x: array([[0.38888889],\n       [2.75      ]])\n y: array([[-126.],\n       [-891.]])"
     ]
    }
   ],
   "source": [
    "########\n",
    "# TEST\n",
    "########\n",
    "x,y = load_regression_data()\n",
    "X = design_matrix(x)\n",
    "alpha0 = np.array([1,1]).reshape(2,1)\n",
    "np.testing.assert_almost_equal(0.5138888888888888, lin_regression_cost(alpha0, X=X, y=y))\n",
    "np.testing.assert_array_equal((2,1), lin_regression_gradient_cost(alpha0, X=X, y=y).shape)\n",
    "np.testing.assert_array_almost_equal(np.array([0.38888889,2.75]).reshape(2,1), lin_regression_gradient_cost(alpha0, X=X, y=y), decimal=8)\n",
    "alpha, _, _ = vanilla_gradient_descent(lin_regression_cost, lin_regression_gradient_cost, 0.001, alpha0, 200000, eps=1.0e-6, X=X, y=y)\n",
    "np.testing.assert_array_equal((2,1), alpha.shape)\n",
    "np.testing.assert_array_almost_equal(optimized_parameters_exact(X, y), alpha, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">EXERCISE 3 </span>\n",
    "\n",
    "Apply Vanilla Gradient Descent to the linear regression use case. \n",
    "Examine the number of iterations as a function of the learning rate: Choose the fixed starting value $(1,1)$ and an accuracy $\\epsilon = 1.0e^{-6}$.\n",
    "1. Determine the range $[\\rho_{\\rm min}, \\rho_{\\rm max}]$ from which the learning rate can be chosen at which Vanilla Gradient Descent converges. \n",
    "2. Determine the optimal learning rate $\\rho_{\\rm opt}$ at which the number of iterations becomes minimal.\n",
    "3. What happens at learning rates larger than $\\rho_{\\rm opt}$? Explain - also by studying the learning speed curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01000\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 6149, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.02000\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 3072, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.05000\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 1226, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.06000\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 1021, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.06200\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 988, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.06300\n",
      "Optimized Parameters: y-offset 1.32539, slope 0.85714\n",
      "Difference to exact solution: 0.00001\n",
      "Number of iterations: 972, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.06350\n",
      "Optimized Parameters: y-offset 1.32540, slope 0.85714\n",
      "Difference to exact solution: 0.00000\n",
      "Number of iterations: 6409, final cost: 0.38073, final speed: 0.00000\n",
      "Learning Rate: 0.06400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinm/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in matmul\n",
      "/Users/martinm/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in matmul\n",
      "/Users/martinm/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: y-offset    nan, slope    nan\n",
      "Difference to exact solution:    nan\n",
      "Number of iterations: 200000, final cost:    nan, final speed:    nan\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE 3.1\n",
    "\n",
    "x,y = load_regression_data()\n",
    "X = design_matrix(x)\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "* \n",
    "*\n",
    "*\n",
    "*\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:**\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3:**\n",
    "\n",
    "Answer: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application to Rosenbrock Function\n",
    "\n",
    "Rosenbrock function is defined by\n",
    "\n",
    "$\\qquad f(x,y)= \\frac{1}{2}\\left((4(x^2-y))^2+(y-1)^2\\right)$\n",
    "\n",
    "\n",
    "#### <span style=\"color:blue\">EXERCISE 4</span>\n",
    "\n",
    "Implement Vanilla Gradient Descent applied to Rosenbrock's function: Implement the function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- a 2d np-array (of shape (2,1)) \n",
    "\n",
    "    Returns:\n",
    "    c -- value of the rosenbrock function\n",
    "    \"\"\"    \n",
    "    # START YOUR CODE  \n",
    "\n",
    "    X = x[0]\n",
    "    Y = x[1]\n",
    "    return 0.5 * (16 * (Y - X**2)**2 + (Y - 1)**2)\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    \n",
    "def gradient_rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- a 2d np-array (of shape (2,1)) \n",
    "\n",
    "    Returns:\n",
    "    gradient -- gradient of the rosenbrock function, np-array of shape (2,1)\n",
    "    \"\"\"    \n",
    "    # START YOUR CODE\n",
    "\n",
    "    X = x[0]\n",
    "    Y = x[1]\n",
    "    \n",
    "    gdX = -16 * (Y - X**2) * 2 * X\n",
    "    gdY = 16 * (Y - X**2) + (Y - 1)\n",
    "    \n",
    "    return np.array([gdX, gdY]).reshape(2, 1)\n",
    "    \n",
    "    # END YOUR CODE            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7454a6a3bff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrosenbrock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_rosenbrock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_rosenbrock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36mcompare\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "########\n",
    "# TEST\n",
    "########\n",
    "np.testing.assert_almost_equal(0.0, rosenbrock(np.array([1,1]).reshape(2,1)), decimal=8)\n",
    "np.testing.assert_almost_equal(0.0, rosenbrock(np.array([-1,1]).reshape(2,1)), decimal=8)\n",
    "np.testing.assert_almost_equal(17.0/2.0, rosenbrock(np.array([1,0]).reshape(2,1)), decimal=8)\n",
    "\n",
    "np.testing.assert_almost_equal(np.array([0,0]).reshape(2,1), gradient_rosenbrock(np.array([1,1]).reshape(2,1)), decimal=8)\n",
    "np.testing.assert_almost_equal(np.array([32,-17]).reshape(2,1), gradient_rosenbrock(np.array([1,0]).reshape(2,1)), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">AUFGABE 5</span>\n",
    "\n",
    "Apply Vanilla Gradient Descent as implemented in EXERCISE 1. to Rosenbrock's function.<br>\n",
    "\n",
    "1. Analyse the behavior of GD with different learning rates. Select a fixed initial value $(0.5,0.0)$ and a precision $1.0e^{-6}$. For comparison, prepare a plot with number of iterations needed vs learning rate. What learning rates are possible? Is there an optimal learning rate? **Remark:** Only accept solutions that lead to the minimum in the right half plane (located at $(1,1)$).  \n",
    "\n",
    "2. Analyse the behavior of GD with different learning rates - now selecting $(1.0,0.0)$ as initial value (und again a precision $1.0e^{-6}$). Is there a difference in the behavior? \n",
    "\n",
    "3. Analyse the behavior of GD with varying initial value at a fixed learning rate $\\rho=0.02$. Choose initial values always on the x-axis, i.e. $(x,0)$. Prepare a plot with the number of iterations vs initial values. \n",
    "\n",
    "4. Are there areas in the (x,y)-plane that are particularly problematic to be used for the starting values? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 5.1\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kommentar**:\n",
    "\n",
    "()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 5.2\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "\n",
    "()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 5.3\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 5.4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
